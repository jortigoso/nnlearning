{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnistfromscratchnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic8epk3PgC_i"
      },
      "outputs": [],
      "source": [
        "# Implementation of a NN following the article:\n",
        "# https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605\n",
        "\n",
        "# Additional information taken from:\n",
        "# https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
        "# https://mlfromscratch.com/neural-network-tutorial/#/\n",
        "# https://www.youtube.com/watch?v=NJvojeoTnNM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "%pylab inline "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys9ZXQHvgtPP",
        "outputId": "96290bef-98c6-47ff-d6e1-36d76610a858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "mnist = fetch_openml('mnist_784', cache=False)\n",
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int8')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "# from sklearn import datasets\n",
        "# iris = datasets.load_iris()\n",
        "# X = iris.data.astype('float32')\n",
        "# y = iris.target.astype('int64')\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "xfqvQhyyhAU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train).reshape((-1,28,28))\n",
        "X_test = np.array(X_test).reshape((-1,28,28))\n",
        "y_train = np.array(y_train).reshape((52500,))\n",
        "y_test = np.array(y_test).reshape((17500,))"
      ],
      "metadata": {
        "id": "NpcVr6GHhvmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "imshow(X_train[0])\n",
        "print(y_train[0])\n",
        "print(y_train.shape)\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "UaTnngpkjuJv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "1a0c55bb-6cfe-425e-84ac-5a9c1fa58d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(52500, 28, 28)\n",
            "3\n",
            "(52500,)\n",
            "[3 2 8 ... 1 0 0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theory and Notation\n",
        "- X = Inputs\n",
        "- y = labels\n",
        "- b = bias\n",
        "- W = weights\n",
        "- A = preactivation (W.T*X + b)\n",
        "- H = activation g(a(x)) \n",
        "\n",
        "  $ H = g(A) = g(X\\cdot W+b)$ \n",
        "  \n",
        "  - $a^{(1)} = W^{(1)}x + b^{(1)}$ →→→ $h^{(1)} = g(a^{(1)})$\n",
        "  - $a^{(2)} = W^{(2)}h^{(1)} + b^{(2)}$ →→→ $h^{(L)} = g(a^{(L)})$\n",
        "  - $a^{(L+1)} = W^{(L+1)}h^{(L)} + b^{(L+1)}$"
      ],
      "metadata": {
        "id": "tZ-ko8r6lebv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer Class\n",
        "class Layer:\n",
        "  def __init__(self, neurons):\n",
        "    self.neurons = neurons\n",
        "\n",
        "  def ReLU(self, x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "  def Softmax(self, x):\n",
        "    # Softmax(x_m) =  exp(x_m) / sum_i(exp(x_i))\n",
        "\n",
        "    # \"Standard\" inputs can give inestability since exp(n) can easily overflow\n",
        "    # and log(0) can happen. In case itputs are not normalized (for example\n",
        "    # between 0 and 1) the following trick can be used to give it stability\n",
        "    # \n",
        "    # exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    # probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    exp_scores = np.exp(x)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    return probs\n",
        "  \n",
        "  def dReLU(self, dH, A):\n",
        "    # ReLu derivative can be seen as:\n",
        "    # ReLU is computed for a layer L as ReLU(A_L) = H_L \n",
        "    # so when the update of weights (W) or biases (b) is needed (A including both),\n",
        "    # the derivative of A is needed for updating A (W and b)\n",
        "    # dL/dA = dL/dH * dH/dA = dL/dReLU(A) * dReLU(A)/dA\n",
        "\n",
        "    ## Should it be the same as copyind A instead of dH???? I think so\n",
        "    dA = np.array(dH, copy = True)\n",
        "    dA[A <= 0] = 0\n",
        "    return dA\n",
        "  \n",
        "  def Forward(self, x, W, b, activation):\n",
        "    # Forward pass is basically computing H, A = X*W^T + B\n",
        "    # and finally H = Activation Function (A), \n",
        "    A_curr = np.dot(x, W.T) + b\n",
        "\n",
        "    #Check if this is the same:\n",
        "    # A_curr = np.dot(W, x) + b\n",
        "\n",
        "    if activation == 'ReLU':\n",
        "      H_curr = self.ReLU(A_curr)\n",
        "    elif activation == 'Softmax':\n",
        "      H_curr = self.Softmax(A_curr)\n",
        "\n",
        "    return H_curr, A_curr\n",
        "\n",
        "\n",
        "  ## NOTATION WARNING ###########################################\n",
        "  ## dV => dC/dV, I dont have the partial derivative symbol and #\n",
        "  ## dV means the \"gradient of C in respect to V\"               #\n",
        "  ###############################################################\n",
        "  def Backwards(self, dH_curr, W_curr, A_curr, H_prev, activation):\n",
        "    # For the backwards pass we need to compute the gradients of all\n",
        "    # the paramters: weights(dW), biases (db) and the neuron inputs\n",
        "    # x (dH)\n",
        "    # This is used for updating the params: ex: W_1 = W_1 - lr*dC/dW_1\n",
        "\n",
        "    if activation == 'ReLU':\n",
        "      # dC/dW =dC/dH * dH/dA * dA/dW, since dC/dA = dC/dH * dH/dA\n",
        "      # is computed inside the dReLU function dC/dW can be seen as\n",
        "      # dC/dW = dC/dA * dA/dWm, dA/dW its just A \n",
        "      dA = self.dReLU(dH_curr, A_curr) #dA actually means dC/dA where dC is actually dH really \n",
        "\n",
        "      # For the second term we need to derivate dA/dW and we know \n",
        "      # for an inner layer (only pointing this out because im gonna name the)\n",
        "      # inputs as H (of the prev layer) instead of x)\n",
        "      # A = W*H + B, dA/dW = H \n",
        "      dW = np.dot(H_prev.T, dA)\n",
        "\n",
        "      # For dC/db we just have to take dA into account since\n",
        "      # dA/db = 1\n",
        "      db = np.sum(dA, axis=0, keepdims=True)\n",
        "\n",
        "      # For dC/dH its the same except now we need dA/dH = W\n",
        "      dH = np.dot(dA, W_curr)\n",
        "\n",
        "    elif activation == 'Softmax':\n",
        "      # Softmax function is defined for x_m as\n",
        "      # Softmax(x_m) = exp(x_m)/sum(exp(x_i)), so when obtaining the gradients\n",
        "      # for each parameter we need to derivate respect the specific param.\n",
        "      # changing notation to x => H for inner layer we can say the output of the softmax\n",
        "      # layer is H_curr = exp(h_prev)/sum(exp(sum(h_n))), so without taking into account a cross entropy function:\n",
        "      #\n",
        "      # #REFSOFT #REFSOFT #REFSOFT *****\n",
        "      # dH_curr/dW =  dH_curr/dA * dA/dW\n",
        "      #   For the case where h_n = h_prev\n",
        "      #   dH_curr/dA = d/dA * exp(A)/(exp(A)+exp(A_n)) = (exp(A)*(exp(A)+exp(A_n))/(exp(A)+exp(A_n))^2 - (exp(A)^2)/(exp(A)+exp(A_n))^2\n",
        "      #              = exp(A)/(exp(A)+exp(A_n)) - (exp(A)/(exp(A)+exp(A_m)))^2 \n",
        "      #              = dH_curr - dH_curr^2 = dH_curr(1-dH_curr)\n",
        "      #   For the rest\n",
        "      #   dH_curr/da = -dH_curr*dH_n_curr\n",
        "      #  dA/dW = d/dW (H_prev*W+b) = H_prev\n",
        "      # https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy\n",
        "\n",
        "      # dH_curr in this case is computed outside and refers to dH_curr/dA (i think) #REFSOFT #REFSOFT #REFSOFT\n",
        "      # >> Softmax derivative can be [ exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0)) ]\n",
        "      dW = np.dot(H_prev.T, dH_curr)\n",
        "\n",
        "      # For dH_curr/db, it will be the same except dA/db = 1 so \n",
        "      db = np.sum(dH_curr, axis=0, keepdims=True)\n",
        "\n",
        "      # For dH_curr/dH(_prev) = dH_curr/dA * dA/dH = dH_curr * W_curr\n",
        "      dH = np.dot(dH_curr, W_curr)\n",
        "\n",
        "    return dH, dW, db"
      ],
      "metadata": {
        "id": "s_uG0zjhkAAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Metwork Class\n",
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.network = []      # Layers\n",
        "    self.architecture = [] # Mapping input to output neurons\n",
        "    self.params = []       # W, b\n",
        "    self.memory = []       # H, A\n",
        "    self.gradients = []    # dW, db\n",
        "\n",
        "  def add(self, layer):\n",
        "    # Adds a layer to the network array\n",
        "    self.network.append(layer)\n",
        "\n",
        "  def _compile(self, data):\n",
        "    # Iterates through the neurons array in order to set up the layers mapping\n",
        "    # The mapping is done taking the ouput dimension of the previous layer\n",
        "    # to inputting it to the next layer (output number) and configuring\n",
        "    # the activation function used in the neurons of the layer\n",
        "    for idx, layer in enumerate(self.network):\n",
        "      if idx == 0:\n",
        "        self.architecture.append({'input_dim':  data.shape[1],\n",
        "                                  'output_dim': self.network[idx].neurons,\n",
        "                                  'activation': 'ReLU'})\n",
        "      elif idx > 0 and idx < len(self.network)-1:\n",
        "        self.architecture.append({'input_dim':  self.network[idx-1].neurons,\n",
        "                                  'output_dim': self.network[idx].neurons,\n",
        "                                  'activation': 'ReLU'})\n",
        "      else:\n",
        "        self.architecture.append({'input_dim':  self.network[idx-1].neurons,\n",
        "                                  'output_dim': self.network[idx].neurons,\n",
        "                                  'activation': 'Softmax'})\n",
        "    return self\n",
        "\n",
        "  def _init_weights(self, data):\n",
        "    self._compile(data)\n",
        "\n",
        "    np.random.seed(99)\n",
        "\n",
        "    for i in range(len(self.architecture)):\n",
        "      self.params.append({'W':np.random.uniform(low=-1, high=1, size=(self.architecture[i]['output_dim'], self.architecture[i]['input_dim'])),\n",
        "                          'b':np.zeros((1, self.architecture[i]['output_dim']))})\n",
        "      \n",
        "    return self\n",
        "\n",
        "  def _forwardprop(self, data):\n",
        "    H_curr = data\n",
        "\n",
        "    # print(data)\n",
        "\n",
        "    for i in range(len(self.params)):\n",
        "      H_prev = H_curr \n",
        "      H_curr, A_curr = self.network[i].Forward(x=H_prev, W=self.params[i]['W'], b=self.params[i]['b'], activation=self.architecture[i]['activation'])\n",
        "      self.memory.append({'H': H_prev, 'A': A_curr})  \n",
        "    #Returns neuron output\n",
        "    return H_curr \n",
        "\n",
        "  def _backprop(self, predicted, actual):\n",
        "    num_samples = len(actual)\n",
        "\n",
        "    # In order to get the needed gradient (the one that is passed to the backwards prop\n",
        "    # function as dH_curr) we need to use the predicted (last layer output) \n",
        "    # dL/dH = d\n",
        "    \n",
        "    # This is the derivation for backpropagation from the sofmax layer, its\n",
        "    # only computed once since the softmax activation function is only usyed\n",
        "    # for the final layer, then its passed only once to the for loop below\n",
        "    # (backpropagation loop) since for the next steps dH_prev is updated \n",
        "    # and stops being the original (the one from the softmax) gradient\n",
        "\n",
        "    # The derivation is done with the kronken delta \"trick\", when derivating \n",
        "    # the softmax function in respect to its input (preactivation of the layer, A),\n",
        "    # we obtain what shown on #REFSOFT (search avobe):\n",
        "    # dSoftmax(A_i)/dA = d/dA_j * (exp(A_i)/sum(exp(A)))\n",
        "    #   for i = j\n",
        "    #     d/dA_j = exp(A_i)/(sum(exp(A)))(1-exp(A_j)/(sum(exp(A)))))\n",
        "    #   for i != j\n",
        "    #     d/dA_j = - exp(A_i)/sum(exp(A)) * exp(A_j)/sum(exp(A))\n",
        "    #            =   exp(A_i)/sum(exp(A))(0 - exp(A_j)/sum(exp(A)))\n",
        "    #\n",
        "    # When apllying the \"delta trick\" we just describe the fact that\n",
        "    # for i=j we have \"1\" inside the parenthesis and for i!=j a \"0\"´\n",
        "    # as the konken delta is 1 when evaluated in 0 and 0 elsewhere\n",
        "    # \n",
        "    # In this case the gradient is being calculated taking into account \n",
        "    # that the loss function used is the Cross-Entropy, so we can particularize\n",
        "    # the formula. \n",
        "    # \n",
        "    # dL/dA = dL/dH * dH/dA = dL/dSoftmax * dSoftmax/dA = dL/dYpred * dYpred/dA\n",
        "    #   dL/dYpred = - sum(y_ij/ypred_ij) because of the logarithmic form of the cross entropy (L)\n",
        "    # dL/dA = - sum(y_ij/ypred_ij) * dYpred/dA =   sum(y_ij/ypred_ij) * dSoftmax/dA\n",
        "    #   We knwor from the previous deduction (#REFSOFT) dSoftmax/dA \n",
        "\n",
        "    dscores = predicted\n",
        "    dscores[range(num_samples),actual] -= 1\n",
        "    dscores /= num_samples\n",
        "    \n",
        "    # So apparently this is WRONG since for this \"backwards pass\" to work \n",
        "    # (calculating the gradients of the ouptuts in this way) we need to recieve \n",
        "    # a value procedent form a forward pass which has the categorical Cross-Entropy\n",
        "    # as las step since this derivative is specific for that case\n",
        "    # One-Hot encoding is not needed (nor expected) for \"actual\" or y_true \n",
        "  \n",
        "    #exps = np.exp(predicted - predicted.max())\n",
        "    #dscores = exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "\n",
        "    dH_prev = dscores\n",
        "\n",
        "    for idx, layer in reversed(list(enumerate(self.network))):\n",
        "        dH_curr = dH_prev\n",
        "\n",
        "        H_prev = self.memory[idx]['H']\n",
        "        A_curr = self.memory[idx]['A']\n",
        "        W_curr = self.params[idx]['W']\n",
        "        activation = self.architecture[idx]['activation']\n",
        "\n",
        "        dH_prev, dW_curr, db_curr = layer.Backwards(dH_curr, W_curr, A_curr, H_prev, activation)\n",
        "\n",
        "        self.gradients.append({'dW':dW_curr, 'db':db_curr})\n",
        "        \n",
        "  def _update(self, lr=0.01):\n",
        "    \n",
        "    for idx, layer in enumerate(self.network):\n",
        "      self.params[idx]['W'] -= lr * list(reversed(self.gradients))[idx]['dW'].T  \n",
        "      self.params[idx]['b'] -= lr * list(reversed(self.gradients))[idx]['db']\n",
        "\n",
        "  def _get_accuracy(self, predicted, actual):\n",
        "    return np.mean(np.argmax(predicted, axis=1)==actual)\n",
        "\n",
        "  def _calculate_loss(self, predicted, actual):\n",
        "    samples = len(actual)\n",
        "\n",
        "    # Clipping helps with stability for cases like log(0)\n",
        "    predicted_clipped = np.clip(predicted, 1e-7, 1-1e-7)\n",
        "\n",
        "    correct_logprobs = -np.log(predicted_clipped[range(samples),actual])\n",
        "    data_loss = np.sum(correct_logprobs)/samples\n",
        "\n",
        "    return data_loss\n",
        "  \n",
        "  def train(self, X_train, y_train, epochs):\n",
        "    #SDG\n",
        "    self.loss = []\n",
        "    self.accuracy = []\n",
        "\n",
        "    self._init_weights(X_train)\n",
        "\n",
        "    for i in range(epochs):\n",
        "      yhat = self._forwardprop(X_train)\n",
        "      # print(\"yhat\")\n",
        "      # print(yhat)\n",
        "      self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n",
        "      self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n",
        "\n",
        "      self._backprop(predicted=yhat, actual=y_train)\n",
        "\n",
        "      self._update()\n",
        "\n",
        "      if i % 20 == 0:\n",
        "        s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n",
        "        print(s)"
      ],
      "metadata": {
        "id": "POj9WHKEkHKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network();\n",
        "model.add(Layer(128))\n",
        "model.add(Layer(10))\n",
        "\n",
        "# def one_hot(y, num):\n",
        "#   oh = np.zeros((y.shape[0],num)).astype(np.int8)\n",
        "#   for i, j in enumerate(y):\n",
        "#     oh[i][j] = 1\n",
        "#   return oh\n",
        "\n",
        "#y = LabelEncoder().fit_transform(y_train)\n",
        "# y_train_oh = one_hot(y_train, 10)\n",
        "\n",
        "model.train(X_train.reshape((-1,28*28))/255, y_train, 200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agzTp8sj2Kob",
        "outputId": "8dce1278-7620-4676-ace6-657378d29717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0, ACCURACY: 0.09706666666666666, LOSS: 13.33863014094879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 20, ACCURACY: 0.09883809523809524, LOSS: nan\n",
            "EPOCH: 40, ACCURACY: 0.09883809523809524, LOSS: nan\n",
            "EPOCH: 60, ACCURACY: 0.09883809523809524, LOSS: nan\n",
            "EPOCH: 80, ACCURACY: 0.09883809523809524, LOSS: nan\n"
          ]
        }
      ]
    }
  ]
}
